version: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    depends_on:
      - redis
      - ollama
    environment:
      - NODE_ENV=development
      - PORT=3000
      - REDIS_URL=redis://redis:6379
      - OLLAMA_URL=http://ollama:11434
      - VA_API_BASE_URL=https://sandbox-api.va.gov/services/va_facilities/v1
      - VA_API_KEY=${VA_API_KEY}
      - DEFAULT_MODEL=phi3:mini  # Faster, smaller model for improved response times
      - OPENWEATHERMAP_API_KEY=${OPENWEATHERMAP_API_KEY}
      - GOOGLE_MAPS_API_KEY=${GOOGLE_MAPS_API_KEY}
      - GOOGLE_DIRECTIONS_API_KEY=${GOOGLE_DIRECTIONS_API_KEY}
      - MAPBOX_API_KEY=${MAPBOX_API_KEY}
      - WEATHERAPI_KEY=${WEATHERAPI_KEY}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
      - ./src:/app/src  # Enable hot reload for development
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'  # Reduced for 4-core system
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 45s
      timeout: 15s
      retries: 3
      start_period: 30s

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1  # Limit parallel requests for memory efficiency
      - OLLAMA_MAX_LOADED_MODELS=1  # Only keep one model in memory
    deploy:
      resources:
        limits:
          memory: 10G  # Reserve most memory for LLM
          cpus: '3.0'  # Adjusted for 4-core M2 (leave 1 core for system)
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s

  # Optional: Memory monitor for development
  monitoring:
    image: prom/node-exporter:latest
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($|/)'
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
    profiles:
      - monitoring  # Only start with: docker-compose --profile monitoring up

volumes:
  redis_data:
  ollama_data: